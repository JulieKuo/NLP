{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 400, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 20001     \n",
      "=================================================================\n",
      "Total params: 90,201\n",
      "Trainable params: 90,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#P.279 LSTM的權重參數因多了三個gate，所以是RNN的四倍\n",
    "#RNN的權重(300+1+50) x 50 = 17550\n",
    "#LSTM的權重17550 x 4(三個gate其中一個有兩個) = 70200\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM \n",
    "\n",
    "num_neurons = 50 \n",
    "model = Sequential() \n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "#(300+1+50)*50*4=70200\n",
    "model.add(Dropout(.2))  #shape(400,50)\n",
    "model.add(Flatten()) #shape(0,20000)\n",
    "model.add(Dense(1, activation='sigmoid')) #20000+1=20001\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy']) \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.231 IMDB電影評論dataset的資料前處理\n",
    "#為文檔標記適當label (1(正評) or 0(負評))\n",
    "#混洗(shuffled)所有樣本，使樣本抽出時不會全部正評或負評\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "def pre_process_data(filepath): \n",
    "    \"\"\" ... This is dependent on your training data source but we will\n",
    "    try to generalize it as best as possible. ... \"\"\" \n",
    "    positive_path = os.path.join(filepath, 'pos')#'Documents/Python/NLP/aclImdb/train/pos'\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "    pos_label = 1 \n",
    "    neg_label = 0 \n",
    "    dataset = [] \n",
    "    \n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')): \n",
    "        #*表是該路徑下每個.txt檔案的名稱都跑一次\n",
    "        #glob.glob()返回所有匹配的文件路徑列表\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f: \n",
    "            dataset.append((pos_label, f.read())) \n",
    "            \n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')): \n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f: \n",
    "            dataset.append((neg_label, f.read())) \n",
    "            \n",
    "    shuffle(dataset) \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = pre_process_data('./aclimdb/train') \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.232 對每個文檔裡的每個word做分解\n",
    "#並合併每個word的word_vectors為sample_vecs(文檔的vector)，此時為每個文檔裡的所有word_vectors\n",
    "#再把sample_vecs合併成vectorized_data，此時包含所有文檔的每個word的vector(頻率含意)\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#from nlpia.loaders import get_data \n",
    "#word_vectors = get_data('w2v', limit=200000)\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True , limit=200000)\n",
    "\n",
    "def tokenize_and_vectorize(dataset): \n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    vectorized_data = [] \n",
    "    expected = [] \n",
    "    for sample in dataset: \n",
    "        tokens = tokenizer.tokenize(sample[1])#抓出sample的文字部分做分解\n",
    "        sample_vecs = [] \n",
    "        for token in tokens: \n",
    "            try: \n",
    "                sample_vecs.append(word_vectors[token])\n",
    "                #word_vectors[token]每個字在word_vectors的vector(頻率含意) \n",
    "                \n",
    "            except KeyError: \n",
    "                pass # No matching token in the Google w2v vocab \n",
    "            \n",
    "        vectorized_data.append(sample_vecs) \n",
    "        \n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.233 抓出sample的label部分 \n",
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel off the target values from the dataset \"\"\" \n",
    "    expected = [] \n",
    "    for sample in dataset: \n",
    "        expected.append(sample[0])#label的部分 \n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.234-235 padding(填充)不夠的data或truncate(截斷)多出的data\n",
    "def pad_trunc(data, maxlen): \n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\" \n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors \n",
    "    zero_vector = [] \n",
    "    for _ in range(len(data[0][0])):#每個文檔裡每個word的word_vectors(分數含意)長度都是300  \n",
    "        zero_vector.append(0.0)#生成一個長度為300的zero vectors\n",
    "        \n",
    "    for sample in data:#每個文檔裡的字 \n",
    "        if len(sample) > maxlen:#字的長度大於maxlen=400\n",
    "            temp = sample[:maxlen] #只擷取到maxlen\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample \n",
    "            # Append the appropriate number 0 vectors to the list \n",
    "            additional_elems = maxlen - len(sample) #不足400的長度\n",
    "            for _ in range(additional_elems): \n",
    "                temp.append(zero_vector) #把不足的字的長度以zero vectors加上\n",
    "        else: \n",
    "            temp = sample \n",
    "        new_data.append(temp) \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.285 資料前處理\n",
    "import numpy as np\n",
    "\n",
    "dataset = pre_process_data('./aclimdb/train') \n",
    "vectorized_data = tokenize_and_vectorize(dataset) \n",
    "expected = collect_expected(dataset) \n",
    "\n",
    "split_point = int(len(vectorized_data)*.4)\n",
    "split_point_end = int(len(vectorized_data)*.5)\n",
    "\n",
    "x_train = vectorized_data[:split_point] \n",
    "y_train = expected[:split_point] \n",
    "x_test = vectorized_data[split_point:split_point_end] \n",
    "y_test = expected[split_point:split_point_end]\n",
    "\n",
    "maxlen = 400 \n",
    "batch_size = 32 \n",
    "embedding_dims = 300 \n",
    "epochs = 2\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen) \n",
    "x_test = pad_trunc(x_test, maxlen) \n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims)) \n",
    "y_train = np.array(y_train) \n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims)) \n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 400, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 20001     \n",
      "=================================================================\n",
      "Total params: 90,201\n",
      "Trainable params: 90,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#P.286 建模\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM \n",
    "\n",
    "num_neurons = 50 \n",
    "model = Sequential() \n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "#(300+1+50)*50*4=70200\n",
    "model.add(Dropout(.2))  #shape(400,50)\n",
    "model.add(Flatten()) #shape(0,20000)\n",
    "model.add(Dense(1, activation='sigmoid')) #20000+1=20001\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy']) \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "10000/10000 [==============================] - 257s 26ms/step - loss: 0.5268 - accuracy: 0.7364 - val_loss: 0.5267 - val_accuracy: 0.7388\n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 248s 25ms/step - loss: 0.3933 - accuracy: 0.8245 - val_loss: 0.3723 - val_accuracy: 0.8368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25e25a7b088>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P.286 fit訓練模型\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.286 save model\n",
    "model_structure = model.to_json() \n",
    "with open(\"lstm_model1.json\", \"w\") as json_file: \n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"lstm_weights1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.287\n",
    "from keras.models import model_from_json \n",
    "with open(\"lstm_model1.json\", \"r\") as json_file: \n",
    "    json_string = json_file.read() \n",
    "model = model_from_json(json_string)\n",
    "model.load_weights('lstm_weights1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample's sentiment, 1 - pos, 2 - neg : [[0]]\n",
      "Raw output of sigmoid function: [[0.25997096]]\n"
     ]
    }
   ],
   "source": [
    "#P.287\n",
    "sample_1 = \"\"\"I hate that the dismal weather had me down for so long,\\\n",
    "when will it break! Ugh, when does happiness return? The sun is\\\n",
    "blinding and the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
    "\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "\n",
    "print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model.predict_classes(test_vec))) \n",
    "\n",
    "print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded: 22560\n",
      "Equal: 12\n",
      "Truncated: 2428\n",
      "Avg length: 202.43204\n"
     ]
    }
   ],
   "source": [
    "#P.289 發現token長度設得太高，可調降長度至平均文檔token長度(Avg length)\n",
    "def test_len(data, maxlen): \n",
    "    total_len = truncated = exact = padded = 0 \n",
    "    for sample in data: \n",
    "        total_len += len(sample) \n",
    "        if len(sample) > maxlen: \n",
    "            truncated += 1 \n",
    "        elif len(sample) < maxlen: \n",
    "            padded += 1 \n",
    "        else: \n",
    "            exact +=1 \n",
    "    print('Padded: {}'.format(padded)) \n",
    "    print('Equal: {}'.format(exact)) \n",
    "    print('Truncated: {}'.format(truncated)) \n",
    "    print('Avg length: {}'.format(total_len/len(data)))\n",
    "\n",
    "dataset = pre_process_data('./aclimdb/train') \n",
    "vectorized_data = tokenize_and_vectorize(dataset) \n",
    "test_len(vectorized_data, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.289-290 調降長度至平均文檔token長度\n",
    "import numpy as np \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM \n",
    "\n",
    "maxlen = 200 \n",
    "batch_size = 32 \n",
    "embedding_dims = 300 \n",
    "epochs = 2 \n",
    "num_neurons = 50 \n",
    "dataset = pre_process_data('./aclimdb/train') \n",
    "vectorized_data = tokenize_and_vectorize(dataset) \n",
    "expected = collect_expected(dataset) \n",
    "\n",
    "split_point = int(len(vectorized_data)*.4)\n",
    "split_point_end = int(len(vectorized_data)*.5)\n",
    "x_train = vectorized_data[:split_point] \n",
    "y_train = expected[:split_point] \n",
    "x_test = vectorized_data[split_point:split_point_end] \n",
    "y_test = expected[split_point:split_point_end]\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen) \n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims)) \n",
    "y_train = np.array(y_train) \n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims)) \n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 200, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 80,201\n",
      "Trainable params: 80,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#P.290\n",
    "model = Sequential() \n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims))) \n",
    "#(300+1+50)*50*4=70200\n",
    "model.add(Dropout(.2))  #shape(200,50)\n",
    "model.add(Flatten()) #shape(0,10000)\n",
    "model.add(Dense(1, activation='sigmoid')) #10000+1=10001\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy']) \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 0.5207 - accuracy: 0.7336 - val_loss: 0.4291 - val_accuracy: 0.8144\n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 98s 10ms/step - loss: 0.3952 - accuracy: 0.8288 - val_loss: 0.4828 - val_accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "#P.290 訓練時間少一半，準確度卻沒降很多\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test)) \n",
    "\n",
    "model_structure = model.to_json() \n",
    "with open(\"lstm_model7.json\", \"w\") as json_file: \n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"lstm_weights7.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.293 試著做所有字符的建模，會發現有overfitting的問題\n",
    "#但經過集中培訓的情況下，可對一種特定類型的語言進行建模\n",
    "dataset = pre_process_data('./aclimdb/train') \n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325.06964"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P.293 樣本中平均有多少個token\n",
    "def avg_len(data): \n",
    "    total_len = 0 \n",
    "    for sample in data: \n",
    "        total_len += len(sample[1]) \n",
    "    return total_len/len(data)\n",
    "\n",
    "avg_len(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.293-294 清除與自然語言無關的字母以及無用的標點字符\n",
    "def clean_data(data): \n",
    "    \"\"\"轉換為小寫, 用UNK替代不知道的token\"\"\" \n",
    "    new_data = [] \n",
    "    VALID = 'abcdefghijklmnopqrstuvwxyz0123456789\"\\'?!.,:; ' \n",
    "    for sample in data: \n",
    "        new_sample = [] \n",
    "        for char in sample[1].lower():#取出逐個英文字母 \n",
    "            if char in VALID: #看是否有在VALID\n",
    "                new_sample.append(char) #在的話直接加進list\n",
    "            else: \n",
    "                new_sample.append('UNK') #不在用UNK替代\n",
    "        new_data.append(new_sample) \n",
    "    return new_data\n",
    "\n",
    "listified_data = clean_data(dataset)#取的每個檔案的逐個英文字母的list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.294 填充單個token或truncate\n",
    "def char_pad_trunc(data, maxlen=1500): \n",
    "    \"\"\" We truncate to maxlen or add in PAD tokens \"\"\" \n",
    "    new_dataset = [] \n",
    "    for sample in data: \n",
    "        if len(sample) > maxlen: \n",
    "            new_data = sample[:maxlen] \n",
    "        elif len(sample) < maxlen: \n",
    "            pads = maxlen - len(sample) \n",
    "            new_data = sample + ['PAD'] * pads \n",
    "        else: \n",
    "            new_data = sample \n",
    "        new_dataset.append(new_data) \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.294 為每個字符設indices(索引)\n",
    "def create_dicts(data): \n",
    "    \"\"\" Modified from Keras LSTM example\"\"\" \n",
    "    chars = set() \n",
    "    for sample in data: \n",
    "        chars.update(set(sample))#用set把重複字母刪除，用update()把每個文檔的set添加進chars裡\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars)) \n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars)) \n",
    "    return char_indices, indices_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.295\n",
    "import numpy as np\n",
    "\n",
    "def onehot_encode(dataset, char_indices, maxlen=1500): \n",
    "    \"\"\" One-hot encode the tokens \n",
    "    Args: \n",
    "        dataset list of lists of tokens \n",
    "        char_indices \n",
    "            dictionary of {key=character, value=index to use encoding vector} \n",
    "        maxlen int Length of each sample \n",
    "    Return: \n",
    "        np array of shape (samples, tokens, encoding length) \"\"\" \n",
    "    X = np.zeros((len(dataset), maxlen, len(char_indices.keys()))) \n",
    "    #len(dataset)個maxlen x len(char_indices.keys())的0矩陣\n",
    "    for i, sentence in enumerate(dataset): \n",
    "        for t, char in enumerate(sentence): \n",
    "            X[i, t, char_indices[char]] = 1 \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.295\n",
    "dataset = pre_process_data('./aclimdb/train') \n",
    "expected = collect_expected(dataset) \n",
    "listified_data = clean_data(dataset)\n",
    "\n",
    "common_length_data = char_pad_trunc(listified_data, maxlen=1500) \n",
    "char_indices, indices_char = create_dicts(common_length_data) \n",
    "encoded_data = onehot_encode(common_length_data, char_indices, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.295\n",
    "split_point = int(len(encoded_data)*.4)\n",
    "split_point_end = int(len(encoded_data)*.5)\n",
    "\n",
    "x_train = encoded_data[:split_point] \n",
    "y_train = expected[:split_point] \n",
    "x_test = encoded_data[split_point:split_point_end] \n",
    "y_test = expected[split_point:split_point_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1500, 40)          14080     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1500, 40)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 60000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 60001     \n",
      "=================================================================\n",
      "Total params: 74,081\n",
      "Trainable params: 74,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#P.296\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, LSTM\n",
    "\n",
    "num_neurons = 40 \n",
    "maxlen = 1500 \n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, len(char_indices.keys())))) \n",
    "#return_sequences=False(默認)，只會返回最後一個hidden layer的output\n",
    "#return_sequences=True，包含全部時間部的hidden layer的output\n",
    "model.add(Dropout(.2)) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy']) \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 392s 39ms/step - loss: 0.7147 - accuracy: 0.5278 - val_loss: 0.6960 - val_accuracy: 0.5276\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 392s 39ms/step - loss: 0.6077 - accuracy: 0.6869 - val_loss: 0.7129 - val_accuracy: 0.5524\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 387s 39ms/step - loss: 0.5184 - accuracy: 0.7615 - val_loss: 0.7654 - val_accuracy: 0.5408\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 383s 38ms/step - loss: 0.4312 - accuracy: 0.8201 - val_loss: 0.7888 - val_accuracy: 0.5560\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 385s 39ms/step - loss: 0.3573 - accuracy: 0.8523 - val_loss: 0.8631 - val_accuracy: 0.5564\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 386s 39ms/step - loss: 0.2894 - accuracy: 0.8891 - val_loss: 0.9554 - val_accuracy: 0.5584\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 385s 39ms/step - loss: 0.2315 - accuracy: 0.9198 - val_loss: 1.0690 - val_accuracy: 0.5544\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 385s 38ms/step - loss: 0.1887 - accuracy: 0.9350 - val_loss: 1.1724 - val_accuracy: 0.5568\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 385s 39ms/step - loss: 0.1481 - accuracy: 0.9535 - val_loss: 1.2964 - val_accuracy: 0.5636\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 385s 39ms/step - loss: 0.1189 - accuracy: 0.9630 - val_loss: 1.4299 - val_accuracy: 0.5616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25e28e8ffc8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P.296\n",
    "batch_size = 32 \n",
    "epochs = 10 \n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.297\n",
    "model_structure = model.to_json() \n",
    "with open(\"char_lstm_model3.json\", \"w\") as json_file: \n",
    "    json_file.write(model_structure) \n",
    "model.save_weights(\"char_lstm_weights3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\i7-870\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P.301\n",
    "from nltk.corpus import gutenberg \n",
    "\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus length: 375542 total chars: 50'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P.301 資料前處理\n",
    "text = '' \n",
    "for txt in gutenberg.fileids(): \n",
    "    if 'shakespeare' in txt: \n",
    "        text += gutenberg.raw(txt).lower() #合併所有text\n",
    "chars = sorted(list(set(text))) #將所有文檔放入set找出使用哪些字符，並改外list排續\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) #為每個字符設indices(索引)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars)) \n",
    "'corpus length: {} total chars: {}'.format(len(text), len(chars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of julius caesar by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter flauius, murellus, and certaine commoners ouer the stage.\n",
      "\n",
      "  flauius. hence: home you idle creatures, get you home:\n",
      "is this a holiday? what, know you not\n",
      "(being mechanicall) you ought not walke\n",
      "vpon a labouring day, without the signe\n",
      "of your profession? speake, what trade art thou?\n",
      "  car. why sir, a carpenter\n",
      "\n",
      "   mur. where is thy leather apron, and thy rule?\n",
      "what dost thou with thy best apparrell on\n"
     ]
    }
   ],
   "source": [
    "#P.302 前500字\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 125168\n"
     ]
    }
   ],
   "source": [
    "#P.302 產生train data\n",
    "maxlen = 40 \n",
    "step = 3 \n",
    "sentences = [] \n",
    "next_chars = [] \n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    #逐步增加三個字符， sentences是從預測字開始的maxlen個字，所以預測字不能超過len(text) - maxlen\n",
    "    sentences.append(text[i: i + maxlen]) #抓出從預測字開始的40個字\n",
    "    next_chars.append(text[i + maxlen])#下一個預測的字是往後的第三個字 \n",
    "print('nb sequences:', len(sentences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.302-303 one-hot\n",
    "import numpy as np\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) \n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences): \n",
    "    for t, char in enumerate(sentence): \n",
    "        X[i, t, char_indices[char]] = 1 \n",
    "        y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#P.303\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras.layers import LSTM \n",
    "from keras.optimizers import RMSprop \n",
    "\n",
    "model = Sequential() \n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)))) #(50+1+128)*128*4=91648\n",
    "model.add(Dense(len(chars))) #output為50-D #(128+1)*50=6450\n",
    "model.add(Activation('softmax')) #利用softmax取得50-D的分布概率\n",
    "optimizer = RMSprop(lr=0.01) #利用權重的最新梯度大小的運行平均值來調整學習率以更新每個權重\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer) \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "125168/125168 [==============================] - 267s 2ms/step - loss: 2.0778\n",
      "Epoch 1/1\n",
      "125168/125168 [==============================] - 265s 2ms/step - loss: 1.7076\n",
      "Epoch 1/1\n",
      "125168/125168 [==============================] - 264s 2ms/step - loss: 1.5941\n",
      "Epoch 1/1\n",
      "125168/125168 [==============================] - 266s 2ms/step - loss: 1.5434\n",
      "Epoch 1/1\n",
      "125168/125168 [==============================] - 262s 2ms/step - loss: 1.5990\n"
     ]
    }
   ],
   "source": [
    "#P.304 每epochs個時期保存一次模型並保持訓練\n",
    "epochs = 1\n",
    "batch_size = 128 \n",
    "\n",
    "model_structure = model.to_json() \n",
    "with open(\"shakes_lstm_model.json\", \"w\") as json_file: \n",
    "    json_file.write(model_structure) \n",
    "for i in range(5): \n",
    "    model.fit(X, y, batch_size=batch_size, epochs=epochs) \n",
    "    #如果停止減少損失，就可以安全地停止過程，並在幾個時期內設置好重量\n",
    "    model.save_weights(\"shakes_lstm_weights_{}.h5\".format(i+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json \n",
    "with open(\"shakes_lstm_model.json\", \"r\") as json_file: \n",
    "    json_string = json_file.read() \n",
    "model = model_from_json(json_string)\n",
    "model.load_weights(\"shakes_lstm_weights_{}.h5\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.305 修改多樣性並預測下個字的index #softmax\n",
    "import random \n",
    "def sample(preds, temperature=1.0): \n",
    "    preds = np.asarray(preds).astype('float64') \n",
    "    preds = np.log(preds) / temperature #總和會變\n",
    "    #flattening(temperature > 1)或sharpening(temperature < 1)概率分布\n",
    "    #<1時，更嚴格地重新創建原始文本；>1會產生更多樣化的結果，學習的模式開始被沖走，趨向於胡說八道\n",
    "    exp_preds = np.exp(preds) #手動softmax\n",
    "    preds = exp_preds / np.sum(exp_preds) \n",
    "    probas = np.random.multinomial(1, preds, 1)#(抽樣次數，抽樣的機率，抽出的數量)\n",
    "    #抽出的是1，其餘49個是0\n",
    "    return np.argmax(probas)#輸出擁有最大值的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" which you deny'd me,\n",
      "for i can raise no\"\n",
      " which you deny'd me,\n",
      "for i can raise not the growne the to so sond the great me to the some the strong the great the strong the strong of the stand and so not the storne and the his brutus, and the the strong the some the strong to the great me to the connertiens and the some the more to the ground the sone the sentle and with a partiens to the ground the strong the will the seete the with the singrons confung, the sent the strones hea\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" which you deny'd me,\n",
      "for i can raise no\"\n",
      " which you deny'd me,\n",
      "for i can raise not and mad the griue and the mort of the eare,\n",
      "thered stand where of me strones\n",
      "\n",
      "   bru.. good with a say the selfe the more in the carsaing and the grocke to withere to the wing it the strones and in my lord\n",
      "\n",
      "   casc. as in a groble, and wimes the 'tis besties it doones dishelles here\n",
      "\n",
      "   car. my entrarie,\n",
      "whose her presse gaue and heare, will the mourt in his in the ground me the storne the stirn\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" which you deny'd me,\n",
      "for i can raise no\"\n",
      " which you deny'd me,\n",
      "for i can raise not ataes, willesonnpe of but\n",
      "\n",
      "   bru. sech sent there, i haue so pond sesctreds,\n",
      "the he do but brutus ligh oitiors spownt. then so betser praday'd nebt's ants\n",
      "gray the grone, in ild\n",
      "loue adpacke:\n",
      "a?\n",
      "ther \n",
      "one queeke, i wis day i spon beleld nobrt,\n",
      "they lowt for itare:\n",
      "and portid for looch spean.\n",
      "\n",
      "  rarb. vnde ppocbe,\n",
      "orachats: thefe haghter towre stall newhis otat vnswell the prsuke and good forfes\n"
     ]
    }
   ],
   "source": [
    "#P.306 生成具有不同多樣性的預測文本\n",
    "import sys \n",
    "start_index = random.randint(0, len(text) - maxlen - 1) \n",
    "for diversity in [0.2, 0.5, 1.0]: \n",
    "    print() \n",
    "    print('----- diversity:', diversity) \n",
    "    generated = '' \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence \n",
    "    print('----- Generating with seed: \"' + sentence + '\"') \n",
    "    sys.stdout.write(generated) #\\n時自動換行\n",
    "    for i in range(400): \n",
    "        x = np.zeros((1, maxlen, len(chars))) \n",
    "        for t, char in enumerate(sentence): \n",
    "            x[0, t, char_indices[char]] = 1. \n",
    "        preds = model.predict(x, verbose=0)[0] \n",
    "        next_index = sample(preds, diversity) \n",
    "        next_char = indices_char[next_index] \n",
    "        generated += next_char \n",
    "        sentence = sentence[1:] + next_char \n",
    "        sys.stdout.write(next_char) \n",
    "        sys.stdout.flush() \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.308\n",
    "from keras.models import Sequential \n",
    "from keras.layers import GRU \n",
    "\n",
    "num_neurons = 40 \n",
    "model = Sequential() \n",
    "model.add(GRU(num_neurons, return_sequences=True, input_shape=X[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P.309\n",
    "from keras.models import Sequential \n",
    "from keras.layers import LSTM \n",
    "\n",
    "num_neurons_2 = 40\n",
    "model = Sequential() \n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape)) \n",
    "model.add(LSTM(num_neurons_2, return_sequences=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
